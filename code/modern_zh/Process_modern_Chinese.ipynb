{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Process Training Data\n",
    "\n",
    "Data source:\n",
    "- [new2016zh](https://github.com/brightmart/nlp_chinese_corpus#2%E6%96%B0%E9%97%BB%E8%AF%AD%E6%96%99json%E7%89%88news2016zh)\n",
    "- [NLPCC 2017 task3](http://tcci.ccf.org.cn/conference/2017/taskdata.php)\n",
    "- [中文短文本摘要数据集](https://www.jianshu.com/p/8f52352f0748?tdsourcetag=s_pcqq_aiomsg)\n",
    "- LCSTS\n",
    "\n",
    "All files are in *raw/*. Processed files are saved in *processed/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New2016zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper\n",
    "helper.process_all_zh_files(\"raw/new2016zh/news2016zh_valid.txt\", \"processed/\", \"valid.txt\")\n",
    "helper.process_all_zh_files(\"raw/new2016zh/news2016zh_train.txt\", \"processed/\", \"train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp2017 text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper\n",
    "helper.process_all_nlpcc_files(\"raw/nlpcc2017textsummarization/train_with_summ.txt\", \"processed/\", \"with.txt\")\n",
    "helper.process_all_nlpcc_files(\"raw/nlpcc2017textsummarization/train_without_summ.txt\", \"processed/\", \"without.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新闻标题数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper \n",
    "helper.process_all_news_files(\"raw/新闻标题数据集/train_text.txt\", \"processed/\", \"news_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper \n",
    "\n",
    "# Needs to transfer all stories in LCSTS into one file first, see steps below (two functions named \"get_lcsts\")\n",
    "helper.process_all_news_files(\"raw/LCSTS/lcsts_story.txt\", \"processed/\", \"lcsts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Files\n",
    "\n",
    "Merged all training files into one, and save it to *cw2vec/input.txt*. Set *convert* to True if convert text to traditional Chinese. Split sentences with *Jieba*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import helper \n",
    "helper.merge_files(\"processed/\", \"cw2vec/input.txt\", convert)  # input：文件夹，目标文件 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with cw2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./cw2vec/word2vec substoke -input cw2vec/input.txt -infeature cw2vec/feature.txt -output cw2vec_result/substoke_out_train -lr 0.025 -dim 100 -ws 5 -epoch 5 -minCount 1 -neg 5 -loss ns -minn 3 -maxn 18 -thread 8 -t 1e-4 -lrUpdateRate 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Summary Training Data\n",
    "\n",
    "All files are in *raw/LCSTS/*, processed files are saved in *summ_processed/*. Set *convert* to True if convert text to traditional Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xmltodict, re\n",
    "\n",
    "# split LSCTS to story and summary\n",
    "def get_lcsts():\n",
    "    files = [\"raw/LCSTS/PART_I.txt\", \"raw/LCSTS/PART_II.txt\"]\n",
    "    \n",
    "    for root in files:\n",
    "        count = 0\n",
    "        fin = open(root)\n",
    "        name = root.split(\".\")[0]\n",
    "        f_src = open(name + \"story.txt\", \"w\")\n",
    "        f_trg = open(name + \"summary.txt\", \"w\")\n",
    "        \n",
    "        xml_str = \"\"\n",
    "        for line in tqdm(fin.readlines(), desc=\"process texts\"):\n",
    "            xml_str += line.strip()\n",
    "            if \"</doc>\" in line.strip():\n",
    "                xml_str = xml_str.replace(\" id=%d\" % count, \"\") \\\n",
    "                            .replace(\"&raquo\", \"：\") \\\n",
    "                            .replace(\"<BR/>\", \"\") \\\n",
    "                            .replace(\"<BR>\", \"\")\n",
    "                summary = re.sub(\"|\\</summary\\>.*\", '', re.sub(\"\\<doc\\>.*?\\<summary\\>\", '',xml_str)).replace(\"\\n\", \"\")\n",
    "                story = re.sub(\"|\\</short_text\\>.*\", '', re.sub(\"\\<doc\\>.*?\\<short_text\\>\", '',xml_str)).replace(\"\\n\", \"\")\n",
    "                f_trg.write(summary + \"\\n\")\n",
    "                f_src.write(story + \"\\n\")\n",
    "            \n",
    "                count += 1\n",
    "                xml_str = \"\"\n",
    "        fin.close()\n",
    "    f_src.close()\n",
    "    f_trg.close()\n",
    "\n",
    "get_lcsts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import xmltodict, re\n",
    "\n",
    "# split LSCTS to story and summary\n",
    "def get_lcsts():\n",
    "    files = [\"raw/LCSTS/PART_III.txt\"]\n",
    "    \n",
    "    for root in files:\n",
    "        count = 0\n",
    "        fin = open(root)\n",
    "        name = root.split(\".\")[0]\n",
    "        f_src = open(name + \"story.txt\", \"w\")\n",
    "        f_trg = open(name + \"summary.txt\", \"w\")\n",
    "        \n",
    "        xml_str = \"\"\n",
    "        for line in tqdm(fin.readlines(), desc=\"process texts\"):\n",
    "            xml_str += line.strip()\n",
    "            if \"</doc>\" in line.strip():\n",
    "                xml_str = xml_str.replace(\" id=%d\" % count, \"\") \\\n",
    "                            .replace(\"&raquo\", \"：\") \\\n",
    "                            .replace(\"<BR/>\", \"\") \\\n",
    "                            .replace(\"<BR>\", \"\")\n",
    "                summary = re.sub(\"|\\</summary\\>.*\", '', re.sub(\"\\<doc\\>.*?\\<summary\\>\", '',xml_str))\n",
    "                story = re.sub(\"|\\</short_text\\>.*\", '', re.sub(\"\\<doc\\>.*?\\<short_text\\>\", '',xml_str))\n",
    "                label = re.sub(\"|\\</human_label\\>.*\", '', re.sub(\"\\<doc\\>.*?\\<human_label\\>\", '',xml_str))\n",
    "                if label in ['3','4','5']:\n",
    "                    f_trg.write(summary.replace(\"\\n\", \"\") + \"\\n\")\n",
    "                    f_src.write(story.replace(\"\\n\", \"\") + \"\\n\")\n",
    "                    count += 1\n",
    "                xml_str = \"\"\n",
    "        fin.close()\n",
    "    f_src.close()\n",
    "    f_trg.close()\n",
    "\n",
    "get_lcsts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400591/2400591 [39:31<00:00, 1012.21it/s]\n",
      "100%|██████████| 10666/10666 [00:10<00:00, 1019.21it/s]\n",
      "100%|██████████| 725/725 [00:00<00:00, 899.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import collections, re\n",
    "import struct\n",
    "import re\n",
    "import json\n",
    "import jieba\n",
    "import opencc\n",
    "from tensorflow.core.example import example_pb2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from utils.regu import filter_sentences\n",
    "\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def process_sample(sample):\n",
    "\n",
    "    res = jieba.lcut(filter_sentences(sample))\n",
    "    new = \"\"\n",
    "    for j in res:\n",
    "        new += j + ' '\n",
    "    return new\n",
    "\n",
    "\n",
    "def write_bin(story, summary, writer):\n",
    "    story = story.encode()\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    summary = summary.encode()\n",
    "    \n",
    "    tf_example = example_pb2.Example()\n",
    "    tf_example.features.feature['story'].bytes_list.value.extend([story])\n",
    "    tf_example.features.feature['summary'].bytes_list.value.extend([summary])\n",
    "    tf_example_str = tf_example.SerializeToString()\n",
    "    str_len = len(tf_example_str)\n",
    "    writer.write(struct.pack('q', str_len))\n",
    "    writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "    \n",
    "    story = story.decode()\n",
    "    summary = summary.decode()\n",
    "\n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def write_json(story, summary, writer):\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    \n",
    "    writer.write(\n",
    "                json.dumps({\n",
    "                    'story': story,\n",
    "                    'summary': summary\n",
    "                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_file(story_root, summary_root, target_name, convert, output_type):\n",
    "    converter = opencc.OpenCC('s2t.json')\n",
    "    story_f = open(story_root)\n",
    "    summary_f = open(summary_root)\n",
    "\n",
    "    vocab_counter = collections.Counter()\n",
    "    \n",
    "    if output_type == \"bin\":\n",
    "        with open(target_name + \".bin\",'wb') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                if convert:\n",
    "                    vocab_counter.update(write_bin(process_sample(converter.convert(sto)), process_sample(converter.convert(summ)), writer))\n",
    "                else:\n",
    "                    vocab_counter.update(write_bin(process_sample(sto), process_sample(summ), writer))\n",
    "    elif output_type == \"json\":\n",
    "        with open(target_name + \".json\",'w') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                if convert:\n",
    "                    vocab_counter.update(write_json(process_sample(converter.convert(sto)), process_sample(converter.convert(summ)), writer))\n",
    "                else:\n",
    "                    vocab_counter.update(write_json(process_sample(sto), process_sample(summ), writer))\n",
    "\n",
    "    with open(target_name + \"_vocab.txt\", 'w') as writer:\n",
    "        for word, count in vocab_counter.most_common(len(vocab_counter)):\n",
    "            writer.write(word + ' ' + str(count) + '\\n')\n",
    "     \n",
    "    story_f.close()\n",
    "    summary_f.close()\n",
    "\n",
    "process_file(\"raw/LCSTS/PART_Istory.txt\", \"raw/LCSTS/PART_Isummary.txt\", \"summ_processed/Part1\", False, output_type=\"json\")\n",
    "process_file(\"raw/LCSTS/PART_IIstory.txt\", \"raw/LCSTS/PART_IIsummary.txt\", \"summ_processed/Part2\", False, output_type=\"json\")\n",
    "process_file(\"raw/LCSTS/PART_IIIstory.txt\", \"raw/LCSTS/PART_IIIsummary.txt\", \"summ_processed/Part3\", False, output_type=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
