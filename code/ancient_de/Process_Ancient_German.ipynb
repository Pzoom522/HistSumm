{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Training Data\n",
    "\n",
    "All files are in *raw/*, processed files are saved in *tokenized/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GerManC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import get_all_files\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re, langid\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZäáàâëéèöòüûßæœÆ0-9\\.’\\,\\:\\-\\&\\s]')\n",
    "\n",
    "\n",
    "def swap_GC(line):\n",
    "    s = line\n",
    "    s = s.replace('/',',')\n",
    "    s = re.sub(r'\\[.*?\\]|\\(.*?\\)','',s)\n",
    "    s = re.sub(r'& c','&c',s)\n",
    "    s = re.sub(r'&c','etc',s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def filter_GC(line):\n",
    "    s = line.strip()\n",
    "    s = re.sub('[\\\"§*„ºç’]','',s)\n",
    "    s = re.sub(r'- ','-',s)\n",
    "    s = re.sub(r'\\'s','',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\.+$|;$|\\?$|\\!$','',s)\n",
    "    s = re.sub(r'^,|,$|^-','',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    \n",
    "    if len(s.split()) == 1:\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "\n",
    "root_path = \"raw/GerManC/\"\n",
    "with open(\"tokenized/GerManC.txt\",'w') as output:\n",
    "    for file_name in tqdm(get_all_files(root_path), desc=\"process files\"):\n",
    "        with open(root_path + file_name, 'r', encoding=\"utf-8-sig\") as f:\n",
    "            text = \"\"\n",
    "            for l in f.readlines():\n",
    "                text += l + ' '\n",
    "\n",
    "            for i in sent_tokenize(swap_GC(text), language='german'):\n",
    "                for p1 in i.split(';'):\n",
    "                    for part in p1.split('?'):\n",
    "                        sentence = filter_GC(part).strip()\n",
    "                        if langid.classify(sentence)[0] != 'de':\n",
    "                            continue\n",
    "                        res = word_tokenize(sentence, language='german')\n",
    "                        if len(res) == 0:\n",
    "                            continue\n",
    "                        for j in res:\n",
    "                            output.write(j + ' ')\n",
    "                        output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import get_all_files\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import langid\n",
    "import re\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜäáàâëéêèöòôüûŭúůßæœÆç0-9\\.’\\,\\:\\-\\&\\s]')\n",
    "\n",
    "def swap_DTA(t):\n",
    "    s = t\n",
    "    s = re.sub(r\"\\[.*?\\]|\\(.*?\\)\",'\\n',s)\n",
    "    # delete redundant\n",
    "    s = re.sub(r\"\\)[\\s\\:]*?\\(\\s+?\\d+|\\,?\\d+\\]|\\d+\\.?\\)|[a-z]\\)|†\\)\\:?\",'\\n',s)\n",
    "    s = re.sub(r\"\\*\\)\",'\\n',s)\n",
    "    s = s.replace('- ','')\n",
    "\n",
    "    s = s.replace('uͤ','ü')\n",
    "    s = s.replace('oͤ','ö')\n",
    "    s = s.replace('aͤ','ä')\n",
    "    s = s.replace('ſ','s')\n",
    "    s = s.replace(r'm̃','mm')\n",
    "    s = s.replace(r'ñ','nn')\n",
    "    s = s.replace(r'ẽ','ee')\n",
    "    s = s.replace(r'ẽ','e')\n",
    "    s = s.replace(r'ı','i')\n",
    "    s = s.replace(r'& c','&c')\n",
    "    s = s.replace(r'&c','etc')\n",
    "    s = s.replace(r'ꝛ c','ꝛc')\n",
    "    s = s.replace(r'ꝛc','etc')\n",
    "    s = s.replace(r'ꝛ','r')\n",
    "    s = s.replace('/',',')\n",
    "    return s\n",
    "\n",
    "def filter_DTA(line):\n",
    "    s = line\n",
    "    \n",
    "    s = re.sub('[\\\"§_—\\(\\)‒*”]','',s)\n",
    "    s = re.sub('Vorrede\\.|Inhalt\\.','',s)\n",
    "    \n",
    "    s = re.sub(r'“',' ',s)\n",
    "    s = re.sub(r'„',' ',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\.+$|;$|\\?$|\\!$','',s)\n",
    "    s = re.sub(r'^,|,$|^-','',s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    \n",
    "    s = re.sub(ac_token,'',s)\n",
    "    s = s.strip()\n",
    "\n",
    "    if len(s.split()) == 1:\n",
    "        return \"\"\n",
    "\n",
    "    return s\n",
    "    \n",
    "root_path = \"raw/dta/\"\n",
    "output = open(\"tokenized/DTA.txt\",'w', encoding=\"utf-8-sig\")\n",
    "\n",
    "for folder in tqdm(get_all_files(root_path), desc=\"process by type\"):\n",
    "    for file in get_all_files(root_path + folder):\n",
    "        with open(root_path + folder + \"/\" + file, 'r', encoding=\"utf-8-sig\") as f:\n",
    "            text = \"\"\n",
    "            for line in f.readlines():\n",
    "                text += line.strip() + ' '\n",
    "            for i in sent_tokenize(swap_DTA(text.strip()), language=\"german\"):\n",
    "                for p1 in i.split(';'):\n",
    "                    for part in p1.split('?'):\n",
    "                        for k in sent_tokenize(filter_DTA(part.strip()), language=\"german\"):   \n",
    "                            if langid.classify(k.strip())[0] != 'de':\n",
    "                                continue\n",
    "                            res = word_tokenize(k, language=\"german\") \n",
    "                            if len(res) == 0:\n",
    "                                continue\n",
    "                            for j in res:\n",
    "                                output.write(j + ' ')\n",
    "                            output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mannheimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.helper import get_all_files\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import langid\n",
    "import re\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜäáàâëéêèöòôüûŭúůßæœÆç0-9\\.’\\,\\:\\-\\&\\s]')\n",
    "\n",
    "def swap_Mannheimer(t):\n",
    "    s = t\n",
    "    s = re.sub(r'& c','&c',s)\n",
    "    s = re.sub(r'&c','etc',s)\n",
    "    s = re.sub(r'ſ','s',s)\n",
    "    s = re.sub(r'm̅','mm',s)\n",
    "    s = re.sub(r'n̄','nn',s)\n",
    "    s = re.sub(r'ā','a',s)\n",
    "    s = re.sub(r'ē','e',s)\n",
    "    s = re.sub(r'r̄','r',s)\n",
    "    return s\n",
    "    \n",
    "def filter_Mannheimer(line):\n",
    "    s = line\n",
    "    \n",
    "    s = re.sub(\"\\(.+?\\)|\\(|\\)|\\d+\\s+?\\)|\\- \",'',s)\n",
    "    s = re.sub(\"[|/„“\\*§\\{\\}]\",'',s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\.+$|;$|\\?$|\\!$','',s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    s = re.sub('!|\\?|;','\\n',s)\n",
    "    s = re.sub(ac_token,'',s)\n",
    "    s = s.strip()\n",
    "\n",
    "    if len(s.split()) == 1:\n",
    "        return \"\"\n",
    "        \n",
    "    return s\n",
    "\n",
    "\n",
    "root_path = \"raw/Mannheimer/\"\n",
    "with open(\"tokenized/Mannheimer.txt\",'w') as output:\n",
    "    for file_name in tqdm(get_all_files(root_path), desc=\"process files\"):\n",
    "        with open(root_path + file_name, 'r', encoding=\"utf-8-sig\") as f:\n",
    "            text = \"\"\n",
    "            for l in f.readlines():\n",
    "                text += l.strip() + ' '\n",
    "            filtered = \"\"\n",
    "            # First 10 lines are document info\n",
    "            count = 0\n",
    "            for i in re.findall(r\"<p.+?</p>\",text):\n",
    "                count += 1\n",
    "                if count > 10:\n",
    "                    filtered += re.sub(r\"<.+?>\",' ',i.replace('=','-')) + '\\n'\n",
    "            for sentence in sent_tokenize(filtered.strip(), language=\"german\"):\n",
    "                for p1 in sentence.split(';'):\n",
    "                    for part in p1.split('?'):\n",
    "                        s = swap_Mannheimer(part.strip())\n",
    "                        if langid.classify(s)[0] != 'de':\n",
    "                            continue\n",
    "                        s = filter_Mannheimer(s)\n",
    "                        res = word_tokenize(s, language=\"german\") \n",
    "                        if len(res) == 0:\n",
    "                            continue\n",
    "                        for j in res:\n",
    "                            output.write(j + ' ')\n",
    "                        output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MercuriusTreebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import get_all_files\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import langid\n",
    "import re\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜäáàâëéêèöòôüûŭúůßæœÆç0-9\\.’\\,\\:\\-\\&\\s]')\n",
    "\n",
    "def swap_Mercurius(word):\n",
    "    word = word.replace(\"&#x00e4;\",'ä')\n",
    "    word = word.replace(\"&#x00df;\",'ß')\n",
    "    word = word.replace(\"&#x00fc;\",'ü')\n",
    "    word = word.replace(\"&#x00f6;\",'ö')\n",
    "    word = word.replace(\"&#x00e6;\",'æ')\n",
    "    word = word.replace(\"&#x00e1;\",'á')\n",
    "    word = word.replace(\"&#x00e0;\",'à')\n",
    "    word = word.replace(\"&#x00e8;\",'è')\n",
    "    word = word.replace(\"&#x00e9;\",'é')\n",
    "    word = word.replace(\"&#x00c6;\",'Æ')\n",
    "    word = word.replace(\"&#x00e2;\",'â')\n",
    "    word = word.replace(\"&#x00dc;\",'Ü')\n",
    "    word = word.replace(\"&#x00c4;\",'Ä')\n",
    "    word = word.replace(\"&amp;\",'')\n",
    "    word = word.replace(\"&apos;\",'')\n",
    "    \n",
    "    return word\n",
    "\n",
    "def filter_Mercurius(line):\n",
    "    s = line.strip()\n",
    "    s = re.sub(r'& c','&c',s)\n",
    "    s = re.sub(r'&c','etc',s)\n",
    "    s = re.sub(r'(\\w)~','\\g<1>\\g<1>',s)\n",
    "    s = re.sub('\\(.+?\\)|\\(|\\)','',s)\n",
    "    s = re.sub('!|;|\\?','\\n',s)\n",
    "    s = re.sub('@','',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\.+$|;$|\\?$|\\!$','',s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    s = re.sub(ac_token,'',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    if len(s.split()) == 1:\n",
    "        return \"\"\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "root_path = \"raw/MercuriusTreebank/\"\n",
    "with open(\"tokenized/MercuriusTreebank.txt\",'w') as output:\n",
    "    for file_name in tqdm(get_all_files(root_path), desc=\"process files\"):\n",
    "        with open(root_path + file_name, 'r', encoding=\"utf-8-sig\") as f:\n",
    "            text = \"\"\n",
    "            for l in f.readlines():\n",
    "                text += l.strip()\n",
    "            for i in re.findall(r\"<s.+?</s>\",text):\n",
    "                sentence = \"\"\n",
    "                for j in re.findall(r\"<t.+?/>\",i):               \n",
    "                    word = re.sub(r\"<t.+?word=\\\"\",'',j)\n",
    "                    word = re.sub(r\"\\\".+?/>\",'',word) \n",
    "                    word = swap_Mercurius(word)\n",
    "                    word = word.replace(\"/\",',')\n",
    "                    sentence += word + ' '\n",
    "                sentence = filter_Mercurius(sentence.strip())\n",
    "                if langid.classify(sentence)[0] != 'de':\n",
    "                    continue\n",
    "                res = word_tokenize(sentence, language=\"german\") \n",
    "                if len(res) == 0:\n",
    "                    continue\n",
    "                for j in res:\n",
    "                    output.write(j + ' ')\n",
    "                output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import merge_files\n",
    "merge_files(\"tokenized/\",\"fastText/data/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fastText/data/fasttext skipgram -input fastText/data/input.txt -output out/nonorm -lr 0.025 -dim 100 -ws 5 -epoch 5 -minCount 1 -neg 5 -loss ns -minn 3 -maxn 18 -thread 8 -t 0.0001 -lrUpdateRate 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Summary Testing Data\n",
    "\n",
    "Normed testing story and summary are in *raw/norm/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 369.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import collections, re\n",
    "import struct\n",
    "import langid\n",
    "import json\n",
    "from tensorflow.core.example import example_pb2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜÖäáàâëéêèöòôüûŭúůíßæœÆç0-9\\.\\,\\:\\-\\&\\s\\?\\!\\;]')\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def filter_sentence(l):\n",
    "    s = l\n",
    "    s = re.sub(r\"\\\"\",'',s) \n",
    "    s = re.sub(r\"/\",',',s)\n",
    "    s = re.sub(r\"\\(.+?\\)\",'',s)\n",
    "    s = re.sub(r\"\\[.+?\\]\",'',s)\n",
    "    s = re.sub(r\"^\\d+\\)\",'',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'\\'s','',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    s = re.sub(ac_token,'',s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = s.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "def process_sample(sample):\n",
    "\n",
    "    res = word_tokenize(filter_sentence(sample), language=\"german\")\n",
    "    new = \"\"\n",
    "    for j in res:\n",
    "        new += j + ' '\n",
    "    return new\n",
    "\n",
    "\n",
    "def write_bin(story, summary, writer):\n",
    "    story = story.encode()\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    summary = abstract.encode()\n",
    "    \n",
    "    tf_example = example_pb2.Example()\n",
    "    tf_example.features.feature['story'].bytes_list.value.extend([story])\n",
    "    tf_example.features.feature['summary'].bytes_list.value.extend([summary])\n",
    "    tf_example_str = tf_example.SerializeToString()\n",
    "    str_len = len(tf_example_str)\n",
    "    writer.write(struct.pack('q', str_len))\n",
    "    writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "    \n",
    "    story = story.decode()\n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def write_json(story, summary, writer):\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    \n",
    "    writer.write(\n",
    "                json.dumps({\n",
    "                    'story': story,\n",
    "                    'summary': summary\n",
    "                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    " \n",
    "    \n",
    "def process_file(story_root, summary_root, target_name, output_type):\n",
    "    \n",
    "    story_f = open(story_root)\n",
    "    summary_f = open(summary_root)\n",
    "\n",
    "    vocab_counter = collections.Counter()\n",
    "    if output_type == \"bin\":\n",
    "        with open(target_name + \".bin\",'wb') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                vocab_counter.update(write_bin(process_sample(sto.lower()), process_sample(summ.lower()), writer))\n",
    "    elif output_type == \"json\":\n",
    "        with open(target_name + \".json\",'w') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                vocab_counter.update(write_json(process_sample(sto.lower()), process_sample(summ.lower()), writer))\n",
    "    \n",
    "    with open(target_name + \"_vocab.txt\", 'w') as writer:\n",
    "        for word, count in vocab_counter.most_common(len(vocab_counter)):\n",
    "            writer.write(word + ' ' + str(count) + '\\n')\n",
    "     \n",
    "    story_f.close()\n",
    "    summary_f.close()\n",
    "\n",
    "process_file(\"raw/norm/story.txt\", \"raw/norm/summary.txt\", \"normed_ancient_de_summ\", output_type=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
