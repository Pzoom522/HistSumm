{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Training Data\n",
    "Data Source:\n",
    "- [news.2013.de.shuffled](www.statmt.org/wmt14/training-monolingual-news-crawl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import langid\n",
    "import re\n",
    "import time, threading\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜÖÓäáàâëéêèöòôóüûŭúůíïğßŞøæœÆç0-9\\.’\\,\\:\\-\\&\\s]')\n",
    "URL = re.compile(r'(URL)?:? ?https?://', re.IGNORECASE)\n",
    "\n",
    "def filter_sentence(l):\n",
    "    s = l\n",
    "    s = re.sub(URL, '', s)\n",
    "    s = re.sub(r'\\'s','',s)\n",
    "    s = re.sub(r\"[\\\"`\\']\",'',s)\n",
    "    s = re.sub(r\"/\",' or ',s)\n",
    "    s = re.sub(r\"\\(.+?\\)\",'',s)\n",
    "    s = re.sub(r\"\\[.+?\\]\",'',s)\n",
    "    s = re.sub(r\"^\\d+\\)\",'',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\.+$|;$|\\?$|\\!$|^,|^:|^-','',s)  # 去除句首结束字符\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(ac_token,'',s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "class Reader(threading.Thread):\n",
    "    def __init__(self, file_name, start_pos, end_pos, output):\n",
    "        super(Reader, self).__init__()\n",
    "        self.file_name = file_name\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.output = output\n",
    "\n",
    "    def run(self):\n",
    "        fd = open(self.file_name, 'r', encoding=\"unicode_escape\",errors=\"ignore\")\n",
    "        if self.start_pos != 0:\n",
    "            fd.seek(self.start_pos-1)\n",
    "            if fd.read(1) != '\\n':\n",
    "                line = fd.readline()\n",
    "                self.start_pos = fd.tell()\n",
    "        fd.seek(self.start_pos)\n",
    "        \n",
    "        while (self.start_pos <= self.end_pos):\n",
    "            line = fd.readline()\n",
    "            for p1 in line.split(';'):\n",
    "                for p2 in p1.split('?'):\n",
    "                    part = p2\n",
    "                    if langid.classify(part.strip())[0] != 'de':\n",
    "                        continue\n",
    "                    l = filter_sentence(part.strip())\n",
    "                    res = word_tokenize(l, language=\"german\") \n",
    "                    if len(res) == 0:\n",
    "                        continue\n",
    "                    for j in res:\n",
    "                        self.output.write(j + ' ')\n",
    "                    self.output.write('\\n')\n",
    "            self.start_pos = fd.tell()\n",
    "\n",
    "class Partition(object):\n",
    "    def __init__(self, file_name, thread_num):\n",
    "        self.file_name = file_name\n",
    "        self.block_num = thread_num\n",
    "\n",
    "    def part(self):\n",
    "        fd = open(self.file_name, 'r')\n",
    "        fd.seek(0, 2)\n",
    "        pos_list = []\n",
    "        file_size = fd.tell()\n",
    "        block_size = file_size/self.block_num\n",
    "        start_pos = 0\n",
    "        for i in range(self.block_num):\n",
    "            if i == self.block_num-1:\n",
    "                end_pos = file_size-1\n",
    "                pos_list.append((start_pos, end_pos))\n",
    "                break\n",
    "            end_pos = start_pos+block_size-1\n",
    "            if end_pos >= file_size:\n",
    "                end_pos = file_size-1\n",
    "            if start_pos >= file_size:\n",
    "                break\n",
    "            pos_list.append((start_pos, end_pos))\n",
    "            start_pos = end_pos+1\n",
    "        fd.close()\n",
    "        return pos_list\n",
    "    \n",
    "file_name = \"raw/news.2013.de.shuffled\"\n",
    "p = Partition(file_name, 8)\n",
    "t = []\n",
    "pos = p.part()\n",
    "thread_num = 8\n",
    "\n",
    "\n",
    "for i in range(thread_num):\n",
    "    output = open(\"processed/\" + str(i) + \".txt\", 'w', encoding=\"utf-8-sig\")\n",
    "    t.append(Reader(file_name, *pos[i], output))\n",
    "    \n",
    "    \n",
    "for i in range(thread_num):\n",
    "    t[i].start()\n",
    "for i in range(thread_num):\n",
    "    t[i].join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import merge_files\n",
    "merge_files(\"processed/\", \"fastText/data/input_cased.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "output = open(\"fastText/data/input.txt\", 'w')\n",
    "with open(\"fastText/data/input_cased.txt\", 'r') as f:\n",
    "    for i in tqdm(f.readlines()):\n",
    "        output.write(i.lower())\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train With Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fastText/data/fasttext skipgram -input fastText/data/input.txt -output out -lr 0.025 -dim 100 -ws 5 -epoch 5 -minCount 1 -neg 5 -loss ns -minn 3 -maxn 18 -thread 8 -t 0.0001 -lrUpdateRate 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Summary Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220887/220887 [20:25<00:00, 180.23it/s]\n",
      "100%|██████████| 10701/10701 [01:05<00:00, 164.40it/s]\n",
      "100%|██████████| 11394/11394 [01:07<00:00, 168.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import collections, re\n",
    "import struct\n",
    "import langid\n",
    "import json\n",
    "from tensorflow.core.example import example_pb2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ac_token = re.compile(r'[^a-zA-ZÄÜÖäáàâëéêèöòôüûŭúůíßæœÆç0-9\\.\\,\\:\\-\\&\\s\\?\\!\\;]')\n",
    "URL = re.compile(r'(URL)?:? ?https?://', re.IGNORECASE)\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def filter_sentence(l):\n",
    "    s = l\n",
    "    s = re.sub(URL, '', s)\n",
    "    s = re.sub(r\"\\\"\",'',s)\n",
    "    s = re.sub(r\"/\",' or ',s)\n",
    "    s = re.sub(r\"\\(.+?\\)\",'',s)\n",
    "    s = re.sub(r\"\\[.+?\\]\",'',s)\n",
    "    s = re.sub(r\"^\\d+\\)\",'',s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'\\'s','',s)\n",
    "    s = re.sub(r'\\'','’',s)\n",
    "    s = s.replace(r'’',' ’ ')\n",
    "    s = re.sub(ac_token,'',s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = s.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "def process_sample(sample):\n",
    "\n",
    "    res = word_tokenize(filter_sentence(sample), language=\"german\")\n",
    "    new = \"\"\n",
    "    for j in res:\n",
    "        new += j + ' '\n",
    "    return new\n",
    "\n",
    "\n",
    "def write_bin(story, summary, writer):\n",
    "    story = story.encode()\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    summary = summary.encode()\n",
    "    \n",
    "    tf_example = example_pb2.Example()\n",
    "    tf_example.features.feature['story'].bytes_list.value.extend([story])\n",
    "    tf_example.features.feature['summary'].bytes_list.value.extend([summary])\n",
    "    tf_example_str = tf_example.SerializeToString()\n",
    "    str_len = len(tf_example_str)\n",
    "    writer.write(struct.pack('q', str_len))\n",
    "    writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "    \n",
    "    story = story.decode()\n",
    "    summary = summary.decode()\n",
    "\n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def write_json(story, summary, writer):\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    \n",
    "    writer.write(\n",
    "                json.dumps({\n",
    "                    'story': story,\n",
    "                    'summary': summary\n",
    "                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    " \n",
    "\n",
    "def process_file(story_root, summary_root, target_name, output_type):\n",
    "    \n",
    "    story_f = open(story_root)\n",
    "    summary_f = open(summary_root)\n",
    "\n",
    "    vocab_counter = collections.Counter()\n",
    "    if output_type == \"bin\":\n",
    "        with open(target_name + \".bin\",'wb') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                vocab_counter.update(write_bin(process_sample(sto.lower()), process_sample(summ.lower()), writer))\n",
    "    elif output_type == \"json\":\n",
    "        with open(target_name + \".json\",'w') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                vocab_counter.update(write_json(process_sample(sto.lower()), process_sample(summ.lower()), writer))\n",
    "    \n",
    "    with open(target_name + \"_vocab.txt\", 'w') as writer:\n",
    "        for word, count in vocab_counter.most_common(len(vocab_counter)):\n",
    "            writer.write(word + ' ' + str(count) + '\\n')\n",
    "     \n",
    "    story_f.close()\n",
    "    summary_f.close()\n",
    "\n",
    "process_file(\"raw/summ/train.txt.src\", \"raw/summ/train.txt.tgt\", \"train\", output_type=\"json\")\n",
    "process_file(\"raw/summ/test.txt.src\", \"raw/summ/test.txt.tgt\", \"test\", output_type=\"json\")\n",
    "process_file(\"raw/summ/val.txt.src\", \"raw/summ/val.txt.tgt\", \"val\", output_type=\"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
