{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Process Training Data\n",
    "\n",
    "## Using Regular Expression and Tag Punctuations\n",
    "Data Source:\n",
    "- [Ming Text](https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset), choose files from \"小说话本\" (novels), \"史书\" (history) and \"兵书\" (military)\n",
    "- *Wanli Gazette* 130 items\n",
    "\n",
    "All data are in *raw/*. Some of the files in **Ming Text** don't have punctuations, split files and tag unmarked ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_data\n",
    "\n",
    "data_path = 'raw/'\n",
    "punctuation_path = 'punc/'\n",
    "\n",
    "split_data.split_marked_unmarked_files(data_path, punctuation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag punctutation with CRFPunctuator from [Jiayan](https://github.com/jiaeyan/Jiayan), files after tagging are saved to *punc/mlmarked_path/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper\n",
    "\n",
    "marked_path = punctuation_path + 'marked/'\n",
    "unmarked_path = punctuation_path + 'unmarked/'\n",
    "mlmarked_path = punctuation_path + 'mlmarked/'\n",
    "\n",
    "helper.tag_text(unmarked_path, mlmarked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter sentences, only save tokens that are ，：、\\u4e00-\\u9fa5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper\n",
    "\n",
    "processed_path = 'processed/'\n",
    "helper.process_file(mlmarked_path, processed_path, False)\n",
    "helper.process_file(marked_path, processed_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with cw2vec\n",
    "Cut sentences into words and train using [cw2vec](https://github.com/bamtercelboo/cw2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helper\n",
    "\n",
    "helper.separate_words(processed_path, \"cw2vec/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./cw2vec/word2vec substoke -input cw2vec/input.txt -infeature cw2vec/feature.txt -output cw2vec_result/substoke_out -lr 0.025 -dim 100 -ws 5 -epoch 5 -minCount 1 -neg 5 -loss ns -minn 3 -maxn 18 -thread 8 -t 1e-4 -lrUpdateRate 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Testing Wanli Data\n",
    "\n",
    "Wanli data are in *wanli_story_path* and *wanli_summary_path*. Saved data are in *ancientChinese.bin* and *ancientChinese_vocab.txt*.\n",
    "\n",
    "Choose to convert to traditional Chinese by setting *convert*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 2550.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import collections, re\n",
    "import struct\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import urllib\n",
    "import w3lib.html\n",
    "import opencc\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "\n",
    "CHINESE_REGEX = re.compile('[\\u4e00-\\u9fa5]')\n",
    "ac_token = re.compile('[^，：、\\u4e00-\\u9fa5。？！；]')\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def filter_sentence(text: str):\n",
    "\n",
    "    text = \"\".join(text.split())\n",
    "    \n",
    "    text = re.sub(\",\",\"，\", text)\n",
    "    text = re.sub(\":\",\"：\", text)\n",
    "    text = text.replace(\"?\",\"？\")\n",
    "    text = text.replace(\"!\",\"！\")\n",
    "    text = re.sub(ac_token, '', text)  # Keep only '，', '、', and '：'\n",
    "    text = re.sub(\"：+，\", \"，\", text)\n",
    "    text = re.sub(\"、+，\", \"，\", text)\n",
    "    text = re.sub(\"、+：\", \"：\", text)\n",
    "    text = re.sub(\"，+：\", \"：\", text)\n",
    "    text = re.sub(\"：+、\", \"：\", text)\n",
    "    text = re.sub(\"，+、\", \"，\", text)\n",
    "    text = re.sub(r\"，+\", '，', text)\n",
    "    text = re.sub(r\"、+\", '、', text)\n",
    "    text = re.sub(r\"：+\", '：', text)\n",
    "\n",
    "    if len(text) == 1:\n",
    "        return \"\"\n",
    "   \n",
    "    return text\n",
    "\n",
    "def process_sample(sample):\n",
    "\n",
    "    res = filter_sentence(sample)\n",
    "    new = \"\"\n",
    "    for j in res:\n",
    "        new += j + ' '\n",
    "    return new\n",
    "\n",
    "\n",
    "def write_bin(story, summary, writer):\n",
    "    story = story.encode()\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    summary = summary.encode()\n",
    "    \n",
    "    tf_example = example_pb2.Example()\n",
    "    tf_example.features.feature['story'].bytes_list.value.extend([story])\n",
    "    tf_example.features.feature['summary'].bytes_list.value.extend([summary])\n",
    "    tf_example_str = tf_example.SerializeToString()\n",
    "    str_len = len(tf_example_str)\n",
    "    writer.write(struct.pack('q', str_len))\n",
    "    writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "    \n",
    "    story = story.decode()\n",
    "    summary = summary.decode()\n",
    "\n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def write_json(story, summary, writer):\n",
    "    \n",
    "    summary = ' '.join([\"%s %s %s\" % (SENTENCE_START, summary, SENTENCE_END)])\n",
    "    \n",
    "    writer.write(\n",
    "                json.dumps({\n",
    "                    'story': story,\n",
    "                    'summary': summary\n",
    "                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    tokens = story.split(' ')\n",
    "    tokens = [t.strip() for t in tokens] # strip\n",
    "    tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_file(story_root, summary_root, convert, target_name, output_type):\n",
    "    converter = opencc.OpenCC('s2t.json')\n",
    "    story_f = open(story_root)\n",
    "    summary_f = open(summary_root)\n",
    "\n",
    "    vocab_counter = collections.Counter()\n",
    "    if output_type == \"bin\":\n",
    "        with open(target_name + \".bin\",'wb') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                if convert:\n",
    "                    vocab_counter.update(write_bin(process_sample(converter.convert(sto)), \n",
    "                                                   process_sample(converter.convert(summ)), writer))\n",
    "                else:\n",
    "                    vocab_counter.update(write_bin(process_sample(sto), \n",
    "                                                   process_sample(summ), writer))\n",
    "    elif output_type == \"json\":\n",
    "        with open(target_name + \".json\",'w') as writer:\n",
    "            for sto in tqdm(story_f.readlines()):\n",
    "                summ = summary_f.readline().strip()\n",
    "                if convert:\n",
    "                    vocab_counter.update(write_json(process_sample(converter.convert(sto)), \n",
    "                                                   process_sample(converter.convert(summ)), writer))\n",
    "                else:\n",
    "                    vocab_counter.update(write_json(process_sample(sto), \n",
    "                                                   process_sample(summ), writer))\n",
    "    \n",
    "    with open(target_name + \"_vocab.txt\", 'w') as writer:\n",
    "        for word, count in vocab_counter.most_common(len(vocab_counter)):\n",
    "            writer.write(word + ' ' + str(count) + '\\n')\n",
    "     \n",
    "    story_f.close()\n",
    "    summary_f.close()\n",
    "\n",
    "wanli_story_path = \"wanli/story.txt\"\n",
    "wanli_summary_path = \"wanli/summary.txt\"\n",
    "convert = True\n",
    "process_file(wanli_story_path, wanli_summary_path, convert, \"ancientChinese\", output_type=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
